{
  "nodes": {
    "https://docs.crawl4ai.com": {
      "source_url": "https://docs.crawl4ai.com",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * Home\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n  * [ðŸš€ðŸ¤– Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper](https://docs.crawl4ai.com/#crawl4ai-open-source-llm-friendly-web-crawler-scraper)\n  * [Quick Start](https://docs.crawl4ai.com/#quick-start)\n  * [Video Tutorial](https://docs.crawl4ai.com/#video-tutorial)\n  * [What Does Crawl4AI Do?](https://docs.crawl4ai.com/#what-does-crawl4ai-do)\n  * [Documentation Structure](https://docs.crawl4ai.com/#documentation-structure)\n  * [How You Can Support](https://docs.crawl4ai.com/#how-you-can-support)\n  * [Quick Links](https://docs.crawl4ai.com/#quick-links)\n\n\n# ðŸš€ðŸ¤– Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper\n[ ![unclecode%2Fcrawl4ai | Trendshift](https://trendshift.io/api/badge/repositories/11716) ](https://trendshift.io/repositories/11716)\n[ ![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social) ](https://github.com/unclecode/crawl4ai/stargazers) [ ![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social) ](https://github.com/unclecode/crawl4ai/network/members) [ ![PyPI version](https://badge.fury.io/py/crawl4ai.svg) ](https://badge.fury.io/py/crawl4ai)\n[ ![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai) ](https://pypi.org/project/crawl4ai/) [ ![Downloads](https://static.pepy.tech/badge/crawl4ai/month) ](https://pepy.tech/project/crawl4ai) [ ![License](https://img.shields.io/github/license/unclecode/crawl4ai) ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)\nCrawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, **Crawl4AI** empowers developers with unmatched speed, precision, and deployment ease.\n> **Note** : If you're looking for the old documentation, you can access it [here](https://old.docs.crawl4ai.com).\n## Quick Start\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n  # Create an instance of AsyncWebCrawler\n  async with AsyncWebCrawler() as crawler:\n    # Run the crawler on a URL\n    result = await crawler.arun(url=\"https://crawl4ai.com\")\n    # Print the extracted content\n    print(result.markdown)\n# Run the async main function\nasyncio.run(main())\nCopy\n```\n\n## Video Tutorial\n## What Does Crawl4AI Do?\nCrawl4AI is a feature-rich crawler and scraper that aims to:\n1. **Generate Clean Markdown** : Perfect for RAG pipelines or direct ingestion into LLMs. 2. **Structured Extraction** : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. **Advanced Browser Control** : Hooks, proxies, stealth modes, session re-useâ€”fine-grained control. 4. **High Performance** : Parallel crawling, chunk-based extraction, real-time use cases. 5. **Open Source** : No forced API keys, no paywallsâ€”everyone can access their data. \n**Core Philosophies** : - **Democratize Data** : Free to use, transparent, and highly configurable. - **LLM Friendly** : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it.\n## Documentation Structure\nTo help you get started, weâ€™ve organized our docs into clear sections:\n  * **Setup & Installation** Basic instructions to install Crawl4AI via pip or Docker. \n  * **Quick Start** A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. \n  * **Core** Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. \n  * **Advanced** Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. \n  * **Extraction** Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. \n  * **API Reference** Find the technical specifics of each class and method, including `AsyncWebCrawler`, `arun()`, and `CrawlResult`.\n\n\nThroughout these sections, youâ€™ll find code samples you can **copy-paste** into your environment. If something is missing or unclear, raise an issue or PR.\n## How You Can Support\n  * **Star & Fork**: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. \n  * **File Issues** : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. \n  * **Pull Requests** : Whether itâ€™s a small fix, a big feature, or better docsâ€”contributions are always welcome. \n  * **Join Discord** : Come chat about web scraping, crawling tips, or AI workflows with the community. \n  * **Spread the Word** : Mention Crawl4AI in your blog posts, talks, or on social media. \n\n\n**Our mission** : to empower everyoneâ€”students, researchers, entrepreneurs, data scientistsâ€”to access, parse, and shape the worldâ€™s data with speed, cost-efficiency, and creative freedom.\n## Quick Links\n  * **[GitHub Repo](https://github.com/unclecode/crawl4ai)**\n  * **[Installation Guide](https://docs.crawl4ai.com/core/installation/)**\n  * **[Quick Start](https://docs.crawl4ai.com/core/quickstart/)**\n  * **[API Reference](https://docs.crawl4ai.com/api/async-webcrawler/)**\n  * **[Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)**\n\n\nThank you for joining me on this journey. Letâ€™s keep building an **open, democratic** approach to data extraction and AI together.\nHappy Crawling! â€” _Unclecode, Founder & Maintainer of Crawl4AI_\n#### On this page\n  * [Quick Start](https://docs.crawl4ai.com/#quick-start)\n  * [Video Tutorial](https://docs.crawl4ai.com/#video-tutorial)\n  * [What Does Crawl4AI Do?](https://docs.crawl4ai.com/#what-does-crawl4ai-do)\n  * [Documentation Structure](https://docs.crawl4ai.com/#documentation-structure)\n  * [How You Can Support](https://docs.crawl4ai.com/#how-you-can-support)\n  * [Quick Links](https://docs.crawl4ai.com/#quick-links)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 0,
      "score": 1.0,
      "keywords": [
        "https",
        "crawl",
        "crawling",
        "ai",
        "core",
        "advanced",
        "extraction",
        "extracted",
        "llm",
        "api",
        "com",
        "crawler",
        "quick",
        "github"
      ]
    },
    "https://docs.crawl4ai.com/core/docker-deployment": {
      "source_url": "https://docs.crawl4ai.com/core/docker-deployment",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/core/docker-deployment/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * Docker Deployment\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n  * [Crawl4AI Docker Guide ðŸ³](https://docs.crawl4ai.com/core/docker-deployment/#crawl4ai-docker-guide)\n  * [Table of Contents](https://docs.crawl4ai.com/core/docker-deployment/#table-of-contents)\n  * [Prerequisites](https://docs.crawl4ai.com/core/docker-deployment/#prerequisites)\n  * [Installation](https://docs.crawl4ai.com/core/docker-deployment/#installation)\n  * [MCP (Model Context Protocol) Support](https://docs.crawl4ai.com/core/docker-deployment/#mcp-model-context-protocol-support)\n  * [Additional API Endpoints](https://docs.crawl4ai.com/core/docker-deployment/#additional-api-endpoints)\n  * [Dockerfile Parameters](https://docs.crawl4ai.com/core/docker-deployment/#dockerfile-parameters)\n  * [Using the API](https://docs.crawl4ai.com/core/docker-deployment/#using-the-api)\n  * [Metrics & Monitoring](https://docs.crawl4ai.com/core/docker-deployment/#metrics-monitoring)\n  * [Server Configuration](https://docs.crawl4ai.com/core/docker-deployment/#server-configuration)\n  * [Getting Help](https://docs.crawl4ai.com/core/docker-deployment/#getting-help)\n  * [Summary](https://docs.crawl4ai.com/core/docker-deployment/#summary)\n\n\n# Crawl4AI Docker Guide ðŸ³\n## Table of Contents\n  * [Prerequisites](https://docs.crawl4ai.com/core/docker-deployment/#prerequisites)\n  * [Installation](https://docs.crawl4ai.com/core/docker-deployment/#installation)\n  * [Option 1: Using Pre-built Docker Hub Images (Recommended)](https://docs.crawl4ai.com/core/docker-deployment/#option-1-using-pre-built-docker-hub-images-recommended)\n  * [Option 2: Using Docker Compose](https://docs.crawl4ai.com/core/docker-deployment/#option-2-using-docker-compose)\n  * [Option 3: Manual Local Build & Run](https://docs.crawl4ai.com/core/docker-deployment/#option-3-manual-local-build--run)\n  * [Dockerfile Parameters](https://docs.crawl4ai.com/core/docker-deployment/#dockerfile-parameters)\n  * [Using the API](https://docs.crawl4ai.com/core/docker-deployment/#using-the-api)\n  * [Playground Interface](https://docs.crawl4ai.com/core/docker-deployment/#playground-interface)\n  * [Python SDK](https://docs.crawl4ai.com/core/docker-deployment/#python-sdk)\n  * [Understanding Request Schema](https://docs.crawl4ai.com/core/docker-deployment/#understanding-request-schema)\n  * [REST API Examples](https://docs.crawl4ai.com/core/docker-deployment/#rest-api-examples)\n  * [Additional API Endpoints](https://docs.crawl4ai.com/core/docker-deployment/#additional-api-endpoints)\n  * [HTML Extraction Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#html-extraction-endpoint)\n  * [Screenshot Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#screenshot-endpoint)\n  * [PDF Export Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#pdf-export-endpoint)\n  * [JavaScript Execution Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#javascript-execution-endpoint)\n  * [Library Context Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#library-context-endpoint)\n  * [MCP (Model Context Protocol) Support](https://docs.crawl4ai.com/core/docker-deployment/#mcp-model-context-protocol-support)\n  * [What is MCP?](https://docs.crawl4ai.com/core/docker-deployment/#what-is-mcp)\n  * [Connecting via MCP](https://docs.crawl4ai.com/core/docker-deployment/#connecting-via-mcp)\n  * [Using with Claude Code](https://docs.crawl4ai.com/core/docker-deployment/#using-with-claude-code)\n  * [Available MCP Tools](https://docs.crawl4ai.com/core/docker-deployment/#available-mcp-tools)\n  * [Testing MCP Connections](https://docs.crawl4ai.com/core/docker-deployment/#testing-mcp-connections)\n  * [MCP Schemas](https://docs.crawl4ai.com/core/docker-deployment/#mcp-schemas)\n  * [Metrics & Monitoring](https://docs.crawl4ai.com/core/docker-deployment/#metrics--monitoring)\n  * [Deployment Scenarios](https://docs.crawl4ai.com/core/docker-deployment/#deployment-scenarios)\n  * [Complete Examples](https://docs.crawl4ai.com/core/docker-deployment/#complete-examples)\n  * [Server Configuration](https://docs.crawl4ai.com/core/docker-deployment/#server-configuration)\n  * [Understanding config.yml](https://docs.crawl4ai.com/core/docker-deployment/#understanding-configyml)\n  * [JWT Authentication](https://docs.crawl4ai.com/core/docker-deployment/#jwt-authentication)\n  * [Configuration Tips and Best Practices](https://docs.crawl4ai.com/core/docker-deployment/#configuration-tips-and-best-practices)\n  * [Customizing Your Configuration](https://docs.crawl4ai.com/core/docker-deployment/#customizing-your-configuration)\n  * [Configuration Recommendations](https://docs.crawl4ai.com/core/docker-deployment/#configuration-recommendations)\n  * [Getting Help](https://docs.crawl4ai.com/core/docker-deployment/#getting-help)\n  * [Summary](https://docs.crawl4ai.com/core/docker-deployment/#summary)\n\n\n## Prerequisites\nBefore we dive in, make sure you have: - Docker installed and running (version 20.10.0 or higher), including `docker compose` (usually bundled with Docker Desktop). - `git` for cloning the repository. - At least 4GB of RAM available for the container (more recommended for heavy use). - Python 3.10+ (if using the Python SDK). - Node.js 16+ (if using the Node.js examples).\n> ðŸ’¡ **Pro tip** : Run `docker info` to check your Docker installation and available resources.\n## Installation\nWe offer several ways to get the Crawl4AI server running. The quickest way is to use our pre-built Docker Hub images.\n### Option 1: Using Pre-built Docker Hub Images (Recommended)\nPull and run images directly from Docker Hub without building locally.\n#### 1. Pull the Image\nOur latest release candidate is `0.6.0-r2`. Images are built with multi-arch manifests, so Docker automatically pulls the correct version for your system.\n```\n# Pull the release candidate (recommended for latest features)\ndocker pull unclecode/crawl4ai:0.6.0-r1\n# Or pull the latest stable version\ndocker pull unclecode/crawl4ai:latest\nCopy\n```\n\n#### 2. Setup Environment (API Keys)\nIf you plan to use LLMs, create a `.llm.env` file in your working directory:\n```\n# Create a .llm.env file with your API keys\ncat > .llm.env << EOL\n# OpenAI\nOPENAI_API_KEY=sk-your-key\n# Anthropic\nANTHROPIC_API_KEY=your-anthropic-key\n# Other providers as needed\n# DEEPSEEK_API_KEY=your-deepseek-key\n# GROQ_API_KEY=your-groq-key\n# TOGETHER_API_KEY=your-together-key\n# MISTRAL_API_KEY=your-mistral-key\n# GEMINI_API_TOKEN=your-gemini-token\nEOL\nCopy\n```\n\n> ðŸ”‘ **Note** : Keep your API keys secure! Never commit `.llm.env` to version control.\n#### 3. Run the Container\n  * **Basic run:**\n```\ndocker run -d \\\n -p 11235:11235 \\\n --name crawl4ai \\\n --shm-size=1g \\\n unclecode/crawl4ai:latest\nCopy\n```\n\n  * **With LLM support:**\n```\n# Make sure .llm.env is in the current directory\ndocker run -d \\\n -p 11235:11235 \\\n --name crawl4ai \\\n --env-file .llm.env \\\n --shm-size=1g \\\n unclecode/crawl4ai:latest\nCopy\n```\n\n\n\n> The server will be available at `http://localhost:11235`. Visit `/playground` to access the interactive testing interface.\n#### 4. Stopping the Container\n```\ndocker stop crawl4ai && docker rm crawl4ai\nCopy\n```\n\n#### Docker Hub Versioning Explained\n  * **Image Name:** `unclecode/crawl4ai`\n  * **Tag Format:** `LIBRARY_VERSION[-SUFFIX]` (e.g., `0.6.0-r2`)\n    * `LIBRARY_VERSION`: The semantic version of the core `crawl4ai` Python library\n    * `SUFFIX`: Optional tag for release candidates (``) and revisions (`r1`)\n  * **`latest`Tag:** Points to the most recent stable version\n  * **Multi-Architecture Support:** All images support both `linux/amd64` and `linux/arm64` architectures through a single tag\n\n\n### Option 2: Using Docker Compose\nDocker Compose simplifies building and running the service, especially for local development and testing.\n#### 1. Clone Repository\n```\ngit clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\nCopy\n```\n\n#### 2. Environment Setup (API Keys)\nIf you plan to use LLMs, copy the example environment file and add your API keys. This file should be in the **project root directory**.\n```\n# Make sure you are in the 'crawl4ai' root directory\ncp deploy/docker/.llm.env.example .llm.env\n# Now edit .llm.env and add your API keys\nCopy\n```\n\n#### 3. Build and Run with Compose\nThe `docker-compose.yml` file in the project root provides a simplified approach that automatically handles architecture detection using buildx.\n  * **Run Pre-built Image from Docker Hub:**\n```\n# Pulls and runs the release candidate from Docker Hub\n# Automatically selects the correct architecture\nIMAGE=unclecode/crawl4ai:latest docker compose up -d\nCopy\n```\n\n  * **Build and Run Locally:**\n```\n# Builds the image locally using Dockerfile and runs it\n# Automatically uses the correct architecture for your machine\ndocker compose up --build -d\nCopy\n```\n\n  * **Customize the Build:**\n```\n# Build with all features (includes torch and transformers)\nINSTALL_TYPE=all docker compose up --build -d\n# Build with GPU support (for AMD64 platforms)\nENABLE_GPU=true docker compose up --build -d\nCopy\n```\n\n\n\n> The server will be available at `http://localhost:11235`.\n#### 4. Stopping the Service\n```\n# Stop the service\ndocker compose down\nCopy\n```\n\n### Option 3: Manual Local Build & Run\nIf you prefer not to use Docker Compose for direct control over the build and run process.\n#### 1. Clone Repository & Setup Environment\nFollow steps 1 and 2 from the Docker Compose section above (clone repo, `cd crawl4ai`, create `.llm.env` in the root).\n#### 2. Build the Image (Multi-Arch)\nUse `docker buildx` to build the image. Crawl4AI now uses buildx to handle multi-architecture builds automatically.\n```\n# Make sure you are in the 'crawl4ai' root directory\n# Build for the current architecture and load it into Docker\ndocker buildx build -t crawl4ai-local:latest --load .\n# Or build for multiple architectures (useful for publishing)\ndocker buildx build --platform linux/amd64,linux/arm64 -t crawl4ai-local:latest --load .\n# Build with additional options\ndocker buildx build \\\n --build-arg INSTALL_TYPE=all \\\n --build-arg ENABLE_GPU=false \\\n -t crawl4ai-local:latest --load .\nCopy\n```\n\n#### 3. Run the Container\n  * **Basic run (no LLM support):**\n```\ndocker run -d \\\n -p 11235:11235 \\\n --name crawl4ai-standalone \\\n --shm-size=1g \\\n crawl4ai-local:latest\nCopy\n```\n\n  * **With LLM support:**\n```\n# Make sure .llm.env is in the current directory (project root)\ndocker run -d \\\n -p 11235:11235 \\\n --name crawl4ai-standalone \\\n --env-file .llm.env \\\n --shm-size=1g \\\n crawl4ai-local:latest\nCopy\n```\n\n\n\n> The server will be available at `http://localhost:11235`.\n#### 4. Stopping the Manual Container\n```\ndocker stop crawl4ai-standalone && docker rm crawl4ai-standalone\nCopy\n```\n\n## MCP (Model Context Protocol) Support\nCrawl4AI server includes support for the Model Context Protocol (MCP), allowing you to connect the server's capabilities directly to MCP-compatible clients like Claude Code.\n### What is MCP?\nMCP is an open protocol that standardizes how applications provide context to LLMs. It allows AI models to access external tools, data sources, and services through a standardized interface.\n### Connecting via MCP\nThe Crawl4AI server exposes two MCP endpoints:\n  * **Server-Sent Events (SSE)** : `http://localhost:11235/mcp/sse`\n  * **WebSocket** : `ws://localhost:11235/mcp/ws`\n\n\n### Using with Claude Code\nYou can add Crawl4AI as an MCP tool provider in Claude Code with a simple command:\n```\n# Add the Crawl4AI server as an MCP provider\nclaude mcp add --transport sse c4ai-sse http://localhost:11235/mcp/sse\n# List all MCP providers to verify it was added\nclaude mcp list\nCopy\n```\n\nOnce connected, Claude Code can directly use Crawl4AI's capabilities like screenshot capture, PDF generation, and HTML processing without having to make separate API calls.\n### Available MCP Tools\nWhen connected via MCP, the following tools are available:\n  * `md` - Generate markdown from web content\n  * `html` - Extract preprocessed HTML\n  * `screenshot` - Capture webpage screenshots\n  * `pdf` - Generate PDF documents\n  * `execute_js` - Run JavaScript on web pages\n  * `crawl` - Perform multi-URL crawling\n  * `ask` - Query the Crawl4AI library context\n\n\n### Testing MCP Connections\nYou can test the MCP WebSocket connection using the test file included in the repository:\n```\n# From the repository root\npython tests/mcp/test_mcp_socket.py\nCopy\n```\n\n### MCP Schemas\nAccess the MCP tool schemas at `http://localhost:11235/mcp/schema` for detailed information on each tool's parameters and capabilities.\n## Additional API Endpoints\nIn addition to the core `/crawl` and `/crawl/stream` endpoints, the server provides several specialized endpoints:\n### HTML Extraction Endpoint\n```\nPOST /html\nCopy\n```\n\nCrawls the URL and returns preprocessed HTML optimized for schema extraction.\n```\n{\n \"url\": \"https://example.com\"\n}\nCopy\n```\n\n### Screenshot Endpoint\n```\nPOST /screenshot\nCopy\n```\n\nCaptures a full-page PNG screenshot of the specified URL.\n```\n{\n \"url\": \"https://example.com\",\n \"screenshot_wait_for\": 2,\n \"output_path\": \"/path/to/save/screenshot.png\"\n}\nCopy\n```\n\n  * `screenshot_wait_for`: Optional delay in seconds before capture (default: 2)\n  * `output_path`: Optional path to save the screenshot (recommended)\n\n\n### PDF Export Endpoint\n```\nPOST /pdf\nCopy\n```\n\nGenerates a PDF document of the specified URL.\n```\n{\n \"url\": \"https://example.com\",\n \"output_path\": \"/path/to/save/document.pdf\"\n}\nCopy\n```\n\n  * `output_path`: Optional path to save the PDF (recommended)\n\n\n### JavaScript Execution Endpoint\n```\nPOST /execute_js\nCopy\n```\n\nExecutes JavaScript snippets on the specified URL and returns the full crawl result.\n```\n{\n \"url\": \"https://example.com\",\n \"scripts\": [\n  \"return document.title\",\n  \"return Array.from(document.querySelectorAll('a')).map(a => a.href)\"\n ]\n}\nCopy\n```\n\n  * `scripts`: List of JavaScript snippets to execute sequentially\n\n\n## Dockerfile Parameters\nYou can customize the image build process using build arguments (`--build-arg`). These are typically used via `docker buildx build` or within the `docker-compose.yml` file.\n```\n# Example: Build with 'all' features using buildx\ndocker buildx build \\\n --platform linux/amd64,linux/arm64 \\\n --build-arg INSTALL_TYPE=all \\\n -t yourname/crawl4ai-all:latest \\\n --load \\\n . # Build from root context\nCopy\n```\n\n### Build Arguments Explained\nArgument | Description | Default | Options  \n---|---|---|---  \nINSTALL_TYPE | Feature set | `default` | `default`, `all`, `torch`, `transformer`  \nENABLE_GPU | GPU support (CUDA for AMD64) | `false` | `true`, `false`  \nAPP_HOME | Install path inside container (advanced) | `/app` | any valid path  \nUSE_LOCAL | Install library from local source | `true` | `true`, `false`  \nGITHUB_REPO | Git repo to clone if USE_LOCAL=false | _(see Dockerfile)_ | any git URL  \nGITHUB_BRANCH | Git branch to clone if USE_LOCAL=false | `main` | any branch name  \n_(Note: PYTHON_VERSION is fixed by the`FROM` instruction in the Dockerfile)_\n### Build Best Practices\n  1. **Choose the Right Install Type**\n     * `default`: Basic installation, smallest image size. Suitable for most standard web scraping and markdown generation.\n     * `all`: Full features including `torch` and `transformers` for advanced extraction strategies (e.g., CosineStrategy, certain LLM filters). Significantly larger image. Ensure you need these extras.\n  2. **Platform Considerations**\n     * Use `buildx` for building multi-architecture images, especially for pushing to registries.\n     * Use `docker compose` profiles (`local-amd64`, `local-arm64`) for easy platform-specific local builds.\n  3. **Performance Optimization**\n     * The image automatically includes platform-specific optimizations (OpenMP for AMD64, OpenBLAS for ARM64).\n\n\n## Using the API\nCommunicate with the running Docker server via its REST API (defaulting to `http://localhost:11235`). You can use the Python SDK or make direct HTTP requests.\n### Playground Interface\nA built-in web playground is available at `http://localhost:11235/playground` for testing and generating API requests. The playground allows you to:\n  1. Configure `CrawlerRunConfig` and `BrowserConfig` using the main library's Python syntax\n  2. Test crawling operations directly from the interface\n  3. Generate corresponding JSON for REST API requests based on your configuration\n\n\nThis is the easiest way to translate Python configuration to JSON requests when building integrations.\n### Python SDK\nInstall the SDK: `pip install crawl4ai`\n```\nimport asyncio\nfrom crawl4ai.docker_client import Crawl4aiDockerClient\nfrom crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode # Assuming you have crawl4ai installed\nasync def main():\n  # Point to the correct server port\n  async with Crawl4aiDockerClient(base_url=\"http://localhost:11235\", verbose=True) as client:\n    # If JWT is enabled on the server, authenticate first:\n    # await client.authenticate(\"user@example.com\") # See Server Configuration section\n    # Example Non-streaming crawl\n    print(\"--- Running Non-Streaming Crawl ---\")\n    results = await client.crawl(\n      [\"https://httpbin.org/html\"],\n      browser_config=BrowserConfig(headless=True), # Use library classes for config aid\n      crawler_config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n    )\n    if results: # client.crawl returns None on failure\n     print(f\"Non-streaming results success: {results.success}\")\n     if results.success:\n       for result in results: # Iterate through the CrawlResultContainer\n         print(f\"URL: {result.url}, Success: {result.success}\")\n    else:\n      print(\"Non-streaming crawl failed.\")\n\n    # Example Streaming crawl\n    print(\"\\n--- Running Streaming Crawl ---\")\n    stream_config = CrawlerRunConfig(stream=True, cache_mode=CacheMode.BYPASS)\n    try:\n      async for result in await client.crawl( # client.crawl returns an async generator for streaming\n        [\"https://httpbin.org/html\", \"https://httpbin.org/links/5/0\"],\n        browser_config=BrowserConfig(headless=True),\n        crawler_config=stream_config\n      ):\n        print(f\"Streamed result: URL: {result.url}, Success: {result.success}\")\n    except Exception as e:\n      print(f\"Streaming crawl failed: {e}\")\n\n    # Example Get schema\n    print(\"\\n--- Getting Schema ---\")\n    schema = await client.get_schema()\n    print(f\"Schema received: {bool(schema)}\") # Print whether schema was received\nif __name__ == \"__main__\":\n  asyncio.run(main())\nCopy\n```\n\n_(SDK parameters like timeout, verify_ssl etc. remain the same)_\n### Second Approach: Direct API Calls\nCrucially, when sending configurations directly via JSON, they **must** follow the `{\"type\": \"ClassName\", \"params\": {...}}` structure for any non-primitive value (like config objects or strategies). Dictionaries must be wrapped as `{\"type\": \"dict\", \"value\": {...}}`.\n_(Keep the detailed explanation of Configuration Structure, Basic Pattern, Simple vs Complex, Strategy Pattern, Complex Nested Example, Quick Grammar Overview, Important Rules, Pro Tip)_\n#### More Examples _(Ensure Schema example uses type/value wrapper)_\n**Advanced Crawler Configuration** _(Keep example, ensure cache_mode uses valid enum value like \"bypass\")_\n**Extraction Strategy**\n```\n{\n  \"crawler_config\": {\n    \"type\": \"CrawlerRunConfig\",\n    \"params\": {\n      \"extraction_strategy\": {\n        \"type\": \"JsonCssExtractionStrategy\",\n        \"params\": {\n          \"schema\": {\n            \"type\": \"dict\",\n            \"value\": {\n              \"baseSelector\": \"article.post\",\n              \"fields\": [\n                {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n                {\"name\": \"content\", \"selector\": \".content\", \"type\": \"html\"}\n              ]\n             }\n          }\n        }\n      }\n    }\n  }\n}\nCopy\n```\n\n**LLM Extraction Strategy** _(Keep example, ensure schema uses type/value wrapper)_ _(Keep Deep Crawler Example)_\n### REST API Examples\nUpdate URLs to use port `11235`.\n#### Simple Crawl\n```\nimport requests\n# Configuration objects converted to the required JSON structure\nbrowser_config_payload = {\n  \"type\": \"BrowserConfig\",\n  \"params\": {\"headless\": True}\n}\ncrawler_config_payload = {\n  \"type\": \"CrawlerRunConfig\",\n  \"params\": {\"stream\": False, \"cache_mode\": \"bypass\"} # Use string value of enum\n}\ncrawl_payload = {\n  \"urls\": [\"https://httpbin.org/html\"],\n  \"browser_config\": browser_config_payload,\n  \"crawler_config\": crawler_config_payload\n}\nresponse = requests.post(\n  \"http://localhost:11235/crawl\", # Updated port\n  # headers={\"Authorization\": f\"Bearer {token}\"}, # If JWT is enabled\n  json=crawl_payload\n)\nprint(f\"Status Code: {response.status_code}\")\nif response.ok:\n  print(response.json())\nelse:\n  print(f\"Error: {response.text}\")\nCopy\n```\n\n#### Streaming Results\n```\nimport json\nimport httpx # Use httpx for async streaming example\nasync def test_stream_crawl(token: str = None): # Made token optional\n  \"\"\"Test the /crawl/stream endpoint with multiple URLs.\"\"\"\n  url = \"http://localhost:11235/crawl/stream\" # Updated port\n  payload = {\n    \"urls\": [\n      \"https://httpbin.org/html\",\n      \"https://httpbin.org/links/5/0\",\n    ],\n    \"browser_config\": {\n      \"type\": \"BrowserConfig\",\n      \"params\": {\"headless\": True, \"viewport\": {\"type\": \"dict\", \"value\": {\"width\": 1200, \"height\": 800}}} # Viewport needs type:dict\n    },\n    \"crawler_config\": {\n      \"type\": \"CrawlerRunConfig\",\n      \"params\": {\"stream\": True, \"cache_mode\": \"bypass\"}\n    }\n  }\n  headers = {}\n  # if token:\n  #  headers = {\"Authorization\": f\"Bearer {token}\"} # If JWT is enabled\n  try:\n    async with httpx.AsyncClient() as client:\n      async with client.stream(\"POST\", url, json=payload, headers=headers, timeout=120.0) as response:\n        print(f\"Status: {response.status_code} (Expected: 200)\")\n        response.raise_for_status() # Raise exception for bad status codes\n        # Read streaming response line-by-line (NDJSON)\n        async for line in response.aiter_lines():\n          if line:\n            try:\n              data = json.loads(line)\n              # Check for completion marker\n              if data.get(\"status\") == \"completed\":\n                print(\"Stream completed.\")\n                break\n              print(f\"Streamed Result: {json.dumps(data, indent=2)}\")\n            except json.JSONDecodeError:\n              print(f\"Warning: Could not decode JSON line: {line}\")\n  except httpx.HTTPStatusError as e:\n     print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n  except Exception as e:\n    print(f\"Error in streaming crawl test: {str(e)}\")\n# To run this example:\n# import asyncio\n# asyncio.run(test_stream_crawl())\nCopy\n```\n\n## Metrics & Monitoring\nKeep an eye on your crawler with these endpoints:\n  * `/health` - Quick health check\n  * `/metrics` - Detailed Prometheus metrics\n  * `/schema` - Full API schema\n\n\nExample health check: \n```\ncurl http://localhost:11235/health\nCopy\n```\n\n_(Deployment Scenarios and Complete Examples sections remain the same, maybe update links if examples moved)_\n## Server Configuration\nThe server's behavior can be customized through the `config.yml` file.\n### Understanding config.yml\nThe configuration file is loaded from `/app/config.yml` inside the container. By default, the file from `deploy/docker/config.yml` in the repository is copied there during the build.\nHere's a detailed breakdown of the configuration options (using defaults from `deploy/docker/config.yml`):\n```\n# Application Configuration\napp:\n title: \"Crawl4AI API\"\n version: \"1.0.0\" # Consider setting this to match library version, e.g., \"0.5.1\"\n host: \"0.0.0.0\"\n port: 8020 # NOTE: This port is used ONLY when running server.py directly. Gunicorn overrides this (see supervisord.conf).\n reload: False # Default set to False - suitable for production\n timeout_keep_alive: 300\n# Default LLM Configuration\nllm:\n provider: \"openai/gpt-4o-mini\"\n api_key_env: \"OPENAI_API_KEY\"\n # api_key: sk-... # If you pass the API key directly then api_key_env will be ignored\n# Redis Configuration (Used by internal Redis server managed by supervisord)\nredis:\n host: \"localhost\"\n port: 6379\n db: 0\n password: \"\"\n # ... other redis options ...\n# Rate Limiting Configuration\nrate_limiting:\n enabled: True\n default_limit: \"1000/minute\"\n trusted_proxies: []\n storage_uri: \"memory://\" # Use \"redis://localhost:6379\" if you need persistent/shared limits\n# Security Configuration\nsecurity:\n enabled: false # Master toggle for security features\n jwt_enabled: false # Enable JWT authentication (requires security.enabled=true)\n https_redirect: false # Force HTTPS (requires security.enabled=true)\n trusted_hosts: [\"*\"] # Allowed hosts (use specific domains in production)\n headers: # Security headers (applied if security.enabled=true)\n  x_content_type_options: \"nosniff\"\n  x_frame_options: \"DENY\"\n  content_security_policy: \"default-src 'self'\"\n  strict_transport_security: \"max-age=63072000; includeSubDomains\"\n# Crawler Configuration\ncrawler:\n memory_threshold_percent: 95.0\n rate_limiter:\n  base_delay: [1.0, 2.0] # Min/max delay between requests in seconds for dispatcher\n timeouts:\n  stream_init: 30.0 # Timeout for stream initialization\n  batch_process: 300.0 # Timeout for non-streaming /crawl processing\n# Logging Configuration\nlogging:\n level: \"INFO\"\n format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n# Observability Configuration\nobservability:\n prometheus:\n  enabled: True\n  endpoint: \"/metrics\"\n health_check:\n  endpoint: \"/health\"\nCopy\n```\n\n_(JWT Authentication section remains the same, just note the default port is now 11235 for requests)_\n_(Configuration Tips and Best Practices remain the same)_\n### Customizing Your Configuration\nYou can override the default `config.yml`.\n#### Method 1: Modify Before Build\n  1. Edit the `deploy/docker/config.yml` file in your local repository clone.\n  2. Build the image using `docker buildx` or `docker compose --profile local-... up --build`. The modified file will be copied into the image.\n\n\n#### Method 2: Runtime Mount (Recommended for Custom Deploys)\n  1. Create your custom configuration file, e.g., `my-custom-config.yml` locally. Ensure it contains all necessary sections.\n  2. Mount it when running the container:\n     * **Using`docker run` :**\n```\n# Assumes my-custom-config.yml is in the current directory\ndocker run -d -p 11235:11235 \\\n --name crawl4ai-custom-config \\\n --env-file .llm.env \\\n --shm-size=1g \\\n -v $(pwd)/my-custom-config.yml:/app/config.yml \\\n unclecode/crawl4ai:latest # Or your specific tag\nCopy\n```\n\n     * **Using`docker-compose.yml` :** Add a `volumes` section to the service definition: \n```\nservices:\n crawl4ai-hub-amd64: # Or your chosen service\n  image: unclecode/crawl4ai:latest\n  profiles: [\"hub-amd64\"]\n  <<: *base-config\n  volumes:\n   # Mount local custom config over the default one in the container\n   - ./my-custom-config.yml:/app/config.yml\n   # Keep the shared memory volume from base-config\n   - /dev/shm:/dev/shm\nCopy\n```\n\n_(Note: Ensure`my-custom-config.yml` is in the same directory as `docker-compose.yml`)_\n\n\n> ðŸ’¡ When mounting, your custom file _completely replaces_ the default one. Ensure it's a valid and complete configuration.\n### Configuration Recommendations\n  1. **Security First** ðŸ”’\n  2. Always enable security in production\n  3. Use specific trusted_hosts instead of wildcards\n  4. Set up proper rate limiting to protect your server\n  5. Consider your environment before enabling HTTPS redirect\n  6. **Resource Management** ðŸ’»\n  7. Adjust memory_threshold_percent based on available RAM\n  8. Set timeouts according to your content size and network conditions\n  9. Use Redis for rate limiting in multi-container setups\n  10. **Monitoring** ðŸ“Š\n  11. Enable Prometheus if you need metrics\n  12. Set DEBUG logging in development, INFO in production\n  13. Regular health check monitoring is crucial\n  14. **Performance Tuning** âš¡\n  15. Start with conservative rate limiter delays\n  16. Increase batch_process timeout for large content\n  17. Adjust stream_init timeout based on initial response times\n\n\n## Getting Help\nWe're here to help you succeed with Crawl4AI! Here's how to get support:\n  * ðŸ“– Check our [full documentation](https://docs.crawl4ai.com)\n  * ðŸ› Found a bug? [Open an issue](https://github.com/unclecode/crawl4ai/issues)\n  * ðŸ’¬ Join our [Discord community](https://discord.gg/crawl4ai)\n  * â­ Star us on GitHub to show support!\n\n\n## Summary\nIn this guide, we've covered everything you need to get started with Crawl4AI's Docker deployment: - Building and running the Docker container - Configuring the environment - Using the interactive playground for testing - Making API requests with proper typing - Using the Python SDK - Leveraging specialized endpoints for screenshots, PDFs, and JavaScript execution - Connecting via the Model Context Protocol (MCP) - Monitoring your deployment\nThe new playground interface at `http://localhost:11235/playground` makes it much easier to test configurations and generate the corresponding JSON for API requests.\nFor AI application developers, the MCP integration allows tools like Claude Code to directly access Crawl4AI's capabilities without complex API handling.\nRemember, the examples in the `examples` folder are your friends - they show real-world usage patterns that you can adapt for your needs.\nKeep exploring, and don't hesitate to reach out if you need help! We're building something amazing together. ðŸš€\nHappy crawling! ðŸ•·ï¸\n#### On this page\n  * [Table of Contents](https://docs.crawl4ai.com/core/docker-deployment/#table-of-contents)\n  * [Prerequisites](https://docs.crawl4ai.com/core/docker-deployment/#prerequisites)\n  * [Installation](https://docs.crawl4ai.com/core/docker-deployment/#installation)\n  * [Option 1: Using Pre-built Docker Hub Images (Recommended)](https://docs.crawl4ai.com/core/docker-deployment/#option-1-using-pre-built-docker-hub-images-recommended)\n  * [1. Pull the Image](https://docs.crawl4ai.com/core/docker-deployment/#1-pull-the-image)\n  * [2. Setup Environment (API Keys)](https://docs.crawl4ai.com/core/docker-deployment/#2-setup-environment-api-keys)\n  * [3. Run the Container](https://docs.crawl4ai.com/core/docker-deployment/#3-run-the-container)\n  * [4. Stopping the Container](https://docs.crawl4ai.com/core/docker-deployment/#4-stopping-the-container)\n  * [Docker Hub Versioning Explained](https://docs.crawl4ai.com/core/docker-deployment/#docker-hub-versioning-explained)\n  * [Option 2: Using Docker Compose](https://docs.crawl4ai.com/core/docker-deployment/#option-2-using-docker-compose)\n  * [1. Clone Repository](https://docs.crawl4ai.com/core/docker-deployment/#1-clone-repository)\n  * [2. Environment Setup (API Keys)](https://docs.crawl4ai.com/core/docker-deployment/#2-environment-setup-api-keys)\n  * [3. Build and Run with Compose](https://docs.crawl4ai.com/core/docker-deployment/#3-build-and-run-with-compose)\n  * [4. Stopping the Service](https://docs.crawl4ai.com/core/docker-deployment/#4-stopping-the-service)\n  * [Option 3: Manual Local Build & Run](https://docs.crawl4ai.com/core/docker-deployment/#option-3-manual-local-build-run)\n  * [1. Clone Repository & Setup Environment](https://docs.crawl4ai.com/core/docker-deployment/#1-clone-repository-setup-environment)\n  * [2. Build the Image (Multi-Arch)](https://docs.crawl4ai.com/core/docker-deployment/#2-build-the-image-multi-arch)\n  * [3. Run the Container](https://docs.crawl4ai.com/core/docker-deployment/#3-run-the-container_1)\n  * [4. Stopping the Manual Container](https://docs.crawl4ai.com/core/docker-deployment/#4-stopping-the-manual-container)\n  * [MCP (Model Context Protocol) Support](https://docs.crawl4ai.com/core/docker-deployment/#mcp-model-context-protocol-support)\n  * [What is MCP?](https://docs.crawl4ai.com/core/docker-deployment/#what-is-mcp)\n  * [Connecting via MCP](https://docs.crawl4ai.com/core/docker-deployment/#connecting-via-mcp)\n  * [Using with Claude Code](https://docs.crawl4ai.com/core/docker-deployment/#using-with-claude-code)\n  * [Available MCP Tools](https://docs.crawl4ai.com/core/docker-deployment/#available-mcp-tools)\n  * [Testing MCP Connections](https://docs.crawl4ai.com/core/docker-deployment/#testing-mcp-connections)\n  * [MCP Schemas](https://docs.crawl4ai.com/core/docker-deployment/#mcp-schemas)\n  * [Additional API Endpoints](https://docs.crawl4ai.com/core/docker-deployment/#additional-api-endpoints)\n  * [HTML Extraction Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#html-extraction-endpoint)\n  * [Screenshot Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#screenshot-endpoint)\n  * [PDF Export Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#pdf-export-endpoint)\n  * [JavaScript Execution Endpoint](https://docs.crawl4ai.com/core/docker-deployment/#javascript-execution-endpoint)\n  * [Dockerfile Parameters](https://docs.crawl4ai.com/core/docker-deployment/#dockerfile-parameters)\n  * [Build Arguments Explained](https://docs.crawl4ai.com/core/docker-deployment/#build-arguments-explained)\n  * [Build Best Practices](https://docs.crawl4ai.com/core/docker-deployment/#build-best-practices)\n  * [Using the API](https://docs.crawl4ai.com/core/docker-deployment/#using-the-api)\n  * [Playground Interface](https://docs.crawl4ai.com/core/docker-deployment/#playground-interface)\n  * [Python SDK](https://docs.crawl4ai.com/core/docker-deployment/#python-sdk)\n  * [Second Approach: Direct API Calls](https://docs.crawl4ai.com/core/docker-deployment/#second-approach-direct-api-calls)\n  * [More Examples (Ensure Schema example uses type/value wrapper)](https://docs.crawl4ai.com/core/docker-deployment/#more-examples-ensure-schema-example-uses-typevalue-wrapper)\n  * [REST API Examples](https://docs.crawl4ai.com/core/docker-deployment/#rest-api-examples)\n  * [Simple Crawl](https://docs.crawl4ai.com/core/docker-deployment/#simple-crawl)\n  * [Streaming Results](https://docs.crawl4ai.com/core/docker-deployment/#streaming-results)\n  * [Metrics & Monitoring](https://docs.crawl4ai.com/core/docker-deployment/#metrics-monitoring)\n  * [Server Configuration](https://docs.crawl4ai.com/core/docker-deployment/#server-configuration)\n  * [Understanding config.yml](https://docs.crawl4ai.com/core/docker-deployment/#understanding-configyml)\n  * [Customizing Your Configuration](https://docs.crawl4ai.com/core/docker-deployment/#customizing-your-configuration)\n  * [Method 1: Modify Before Build](https://docs.crawl4ai.com/core/docker-deployment/#method-1-modify-before-build)\n  * [Method 2: Runtime Mount (Recommended for Custom Deploys)](https://docs.crawl4ai.com/core/docker-deployment/#method-2-runtime-mount-recommended-for-custom-deploys)\n  * [Configuration Recommendations](https://docs.crawl4ai.com/core/docker-deployment/#configuration-recommendations)\n  * [Getting Help](https://docs.crawl4ai.com/core/docker-deployment/#getting-help)\n  * [Summary](https://docs.crawl4ai.com/core/docker-deployment/#summary)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "https",
        "crawl",
        "crawling",
        "crawls",
        "deploy",
        "deploys",
        "copy",
        "copied",
        "docker deployment",
        "ai",
        "examples",
        "example",
        "use",
        "uses",
        "useful",
        "configuration",
        "configure",
        "configurations",
        "configuring",
        "build",
        "building",
        "builds",
        "api",
        "run",
        "running",
        "runs"
      ]
    },
    "https://docs.crawl4ai.com/core/quickstart": {
      "source_url": "https://docs.crawl4ai.com/core/quickstart",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/core/quickstart/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * Quick Start\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n  * [Getting Started with Crawl4AI](https://docs.crawl4ai.com/core/quickstart/#getting-started-with-crawl4ai)\n  * [1. Introduction](https://docs.crawl4ai.com/core/quickstart/#1-introduction)\n  * [2. Your First Crawl](https://docs.crawl4ai.com/core/quickstart/#2-your-first-crawl)\n  * [3. Basic Configuration (Light Introduction)](https://docs.crawl4ai.com/core/quickstart/#3-basic-configuration-light-introduction)\n  * [4. Generating Markdown Output](https://docs.crawl4ai.com/core/quickstart/#4-generating-markdown-output)\n  * [5. Simple Data Extraction (CSS-based)](https://docs.crawl4ai.com/core/quickstart/#5-simple-data-extraction-css-based)\n  * [6. Simple Data Extraction (LLM-based)](https://docs.crawl4ai.com/core/quickstart/#6-simple-data-extraction-llm-based)\n  * [7. Multi-URL Concurrency (Preview)](https://docs.crawl4ai.com/core/quickstart/#7-multi-url-concurrency-preview)\n  * [8. Dynamic Content Example](https://docs.crawl4ai.com/core/quickstart/#8-dynamic-content-example)\n  * [9. Next Steps](https://docs.crawl4ai.com/core/quickstart/#9-next-steps)\n\n\n# Getting Started with Crawl4AI\nWelcome to **Crawl4AI** , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, youâ€™ll:\n  1. Run your **first crawl** using minimal configuration. \n  2. Generate **Markdown** output (and learn how itâ€™s influenced by content filters). \n  3. Experiment with a simple **CSS-based extraction** strategy. \n  4. See a glimpse of **LLM-based extraction** (including open-source and closed-source model options). \n  5. Crawl a **dynamic** page that loads content via JavaScript.\n\n\n## 1. Introduction\nCrawl4AI provides:\n  * An asynchronous crawler, **`AsyncWebCrawler`**.\n  * Configurable browser and run settings via **`BrowserConfig`**and**`CrawlerRunConfig`**.\n  * Automatic HTML-to-Markdown conversion via **`DefaultMarkdownGenerator`**(supports optional filters).\n  * Multiple extraction strategies (LLM-based or â€œtraditionalâ€ CSS/XPath-based).\n\n\nBy the end of this guide, youâ€™ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses â€œLoad Moreâ€ buttons or JavaScript updates.\n## 2. Your First Crawl\nHereâ€™s a minimal Python script that creates an **`AsyncWebCrawler`**, fetches a webpage, and prints the first 300 characters of its Markdown output:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nasync def main():\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    print(result.markdown[:300]) # Print first 300 chars\nif __name__ == \"__main__\":\n  asyncio.run(main())\nCopy\n```\n\n**Whatâ€™s happening?** - **`AsyncWebCrawler`**launches a headless browser (Chromium by default). - It fetches`https://example.com`. - Crawl4AI automatically converts the HTML into Markdown.\nYou now have a simple, working crawl!\n## 3. Basic Configuration (Light Introduction)\nCrawl4AIâ€™s crawler can be heavily customized using two main classes:\n1. **`BrowserConfig`**: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2.**`CrawlerRunConfig`**: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.).\nBelow is an example with minimal usage:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nasync def main():\n  browser_conf = BrowserConfig(headless=True) # or False to see the browser\n  run_conf = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS\n  )\n  async with AsyncWebCrawler(config=browser_conf) as crawler:\n    result = await crawler.arun(\n      url=\"https://example.com\",\n      config=run_conf\n    )\n    print(result.markdown)\nif __name__ == \"__main__\":\n  asyncio.run(main())\nCopy\n```\n\n> IMPORTANT: By default cache mode is set to `CacheMode.ENABLED`. So to have fresh content, you need to set it to `CacheMode.BYPASS`\nWeâ€™ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling.\n## 4. Generating Markdown Output\nBy default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a **markdown generator** or **content filter**.\n  * **`result.markdown`**: The direct HTML-to-Markdown conversion.\n  * **`result.markdown.fit_markdown`**: The same content after applying any configured**content filter** (e.g., `PruningContentFilter`).\n\n\n### Example: Using a Filter with `DefaultMarkdownGenerator`\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nmd_generator = DefaultMarkdownGenerator(\n  content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\nconfig = CrawlerRunConfig(\n  cache_mode=CacheMode.BYPASS,\n  markdown_generator=md_generator\n)\nasync with AsyncWebCrawler() as crawler:\n  result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n  print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n  print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\nCopy\n```\n\n**Note** : If you do **not** specify a content filter or markdown generator, youâ€™ll typically see only the raw Markdown. `PruningContentFilter` may adds around `50ms` in processing time. Weâ€™ll dive deeper into these strategies in a dedicated **Markdown Generation** tutorial.\n## 5. Simple Data Extraction (CSS-based)\nCrawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example:\n> **New!** Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n  html,\n  llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\") # Required for OpenAI\n)\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n  html,\n  llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None) # Not needed for Ollama\n)\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)\nCopy\n```\n\nFor a complete guide on schema generation and advanced usage, see [No-LLM Extraction Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/).\nHere's a basic extraction example:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nasync def main():\n  schema = {\n    \"name\": \"Example Items\",\n    \"baseSelector\": \"div.item\",\n    \"fields\": [\n      {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n      {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n    ]\n  }\n  raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"raw://\" + raw_html,\n      config=CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n      )\n    )\n    # The JSON output is stored in 'extracted_content'\n    data = json.loads(result.extracted_content)\n    print(data)\nif __name__ == \"__main__\":\n  asyncio.run(main())\nCopy\n```\n\n**Why is this helpful?** - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store.\n> Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with `raw://`.\n## 6. Simple Data Extraction (LLM-based)\nFor more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports **open-source** or **closed-source** providers:\n  * **Open-Source Models** (e.g., `ollama/llama3.3`, `no_token`) \n  * **OpenAI Models** (e.g., `openai/gpt-4`, requires `api_token`) \n  * Or any provider supported by the underlying library\n\n\nBelow is an example using **open-source** style (no token) and closed-source:\n```\nimport os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nclass OpenAIModelFee(BaseModel):\n  model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n  input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n  output_fee: str = Field(\n    ..., description=\"Fee for output token for the OpenAI model.\"\n  )\nasync def extract_structured_data_using_llm(\n  provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n  print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n  if api_token is None and provider != \"ollama\":\n    print(f\"API token is required for {provider}. Skipping this example.\")\n    return\n  browser_config = BrowserConfig(headless=True)\n  extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n  if extra_headers:\n    extra_args[\"extra_headers\"] = extra_headers\n  crawler_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    word_count_threshold=1,\n    page_timeout=80000,\n    extraction_strategy=LLMExtractionStrategy(\n      llm_config = LLMConfig(provider=provider,api_token=api_token),\n      schema=OpenAIModelFee.model_json_schema(),\n      extraction_type=\"schema\",\n      instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n      Do not miss any models in the entire content.\"\"\",\n      extra_args=extra_args,\n    ),\n  )\n  async with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(\n      url=\"https://openai.com/api/pricing/\", config=crawler_config\n    )\n    print(result.extracted_content)\nif __name__ == \"__main__\":\n  asyncio.run(\n    extract_structured_data_using_llm(\n      provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n    )\n  )\nCopy\n```\n\n**Whatâ€™s happening?** - We define a Pydantic schema (`PricingInfo`) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the **provider** and **api_token** , you can use local models or a remote API.\n## 7. Multi-URL Concurrency (Preview)\nIf you need to crawl multiple URLs in **parallel** , you can use `arun_many()`. By default, Crawl4AI employs a **MemoryAdaptiveDispatcher** , automatically adjusting concurrency based on system resources. Hereâ€™s a quick glimpse:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nasync def quick_parallel_example():\n  urls = [\n    \"https://example.com/page1\",\n    \"https://example.com/page2\",\n    \"https://example.com/page3\"\n  ]\n  run_conf = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    stream=True # Enable streaming mode\n  )\n  async with AsyncWebCrawler() as crawler:\n    # Stream results as they complete\n    async for result in await crawler.arun_many(urls, config=run_conf):\n      if result.success:\n        print(f\"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}\")\n      else:\n        print(f\"[ERROR] {result.url} => {result.error_message}\")\n    # Or get all results at once (default behavior)\n    run_conf = run_conf.clone(stream=False)\n    results = await crawler.arun_many(urls, config=run_conf)\n    for res in results:\n      if res.success:\n        print(f\"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}\")\n      else:\n        print(f\"[ERROR] {res.url} => {res.error_message}\")\nif __name__ == \"__main__\":\n  asyncio.run(quick_parallel_example())\nCopy\n```\n\nThe example above shows two ways to handle multiple URLs: 1. **Streaming mode** (`stream=True`): Process results as they become available using `async for` 2. **Batch mode** (`stream=False`): Wait for all results to complete\nFor more advanced concurrency (e.g., a **semaphore-based** approach, **adaptive memory usage throttling** , or customized rate limiting), see [Advanced Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/).\n## 8. Dynamic Content Example\nSome sites require multiple â€œpage clicksâ€ or dynamic JavaScript updates. Below is an example showing how to **click** a â€œNext Pageâ€ button and wait for new commits to load on GitHub, using **`BrowserConfig`**and**`CrawlerRunConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nasync def extract_structured_data_using_css_extractor():\n  print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n  schema = {\n    \"name\": \"KidoCode Courses\",\n    \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n    \"fields\": [\n      {\n        \"name\": \"section_title\",\n        \"selector\": \"h3.heading-50\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"section_description\",\n        \"selector\": \".charge-content\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"course_name\",\n        \"selector\": \".text-block-93\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"course_description\",\n        \"selector\": \".course-content-text\",\n        \"type\": \"text\",\n      },\n      {\n        \"name\": \"course_icon\",\n        \"selector\": \".image-92\",\n        \"type\": \"attribute\",\n        \"attribute\": \"src\",\n      },\n    ],\n  }\n  browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n  js_click_tabs = \"\"\"\n  (async () => {\n    const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n    for(let tab of tabs) {\n      tab.scrollIntoView();\n      tab.click();\n      await new Promise(r => setTimeout(r, 500));\n    }\n  })();\n  \"\"\"\n  crawler_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    extraction_strategy=JsonCssExtractionStrategy(schema),\n    js_code=[js_click_tabs],\n  )\n  async with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(\n      url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n    )\n    companies = json.loads(result.extracted_content)\n    print(f\"Successfully extracted {len(companies)} companies\")\n    print(json.dumps(companies[0], indent=2))\nasync def main():\n  await extract_structured_data_using_css_extractor()\nif __name__ == \"__main__\":\n  asyncio.run(main())\nCopy\n```\n\n**Key Points** :\n  * **`BrowserConfig(headless=False)`**: We want to watch it click â€œNext Page.â€\n  * **`CrawlerRunConfig(...)`**: We specify the extraction strategy, pass`session_id` to reuse the same page. \n  * **`js_code`**and**`wait_for`**are used for subsequent pages (`page > 0`) to click the â€œNextâ€ button and wait for new commits to load. \n  * **`js_only=True`**indicates weâ€™re not re-navigating but continuing the existing session.\n  * Finally, we call `kill_session()` to clean up the page and browser session.\n\n\n## 9. Next Steps\nCongratulations! You have:\n  1. Performed a basic crawl and printed Markdown. \n  2. Used **content filters** with a markdown generator. \n  3. Extracted JSON via **CSS** or **LLM** strategies. \n  4. Handled **dynamic** pages with JavaScript triggers.\n\n\nIf youâ€™re ready for more, check out:\n  * **Installation** : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. \n  * **Hooks & Auth**: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. \n  * **Deployment** : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. \n  * **Browser Management** : Delve into user simulation, stealth modes, and concurrency best practices. \n\n\nCrawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!\n#### On this page\n  * [1. Introduction](https://docs.crawl4ai.com/core/quickstart/#1-introduction)\n  * [2. Your First Crawl](https://docs.crawl4ai.com/core/quickstart/#2-your-first-crawl)\n  * [3. Basic Configuration (Light Introduction)](https://docs.crawl4ai.com/core/quickstart/#3-basic-configuration-light-introduction)\n  * [4. Generating Markdown Output](https://docs.crawl4ai.com/core/quickstart/#4-generating-markdown-output)\n  * [Example: Using a Filter with DefaultMarkdownGenerator](https://docs.crawl4ai.com/core/quickstart/#example-using-a-filter-with-defaultmarkdowngenerator)\n  * [5. Simple Data Extraction (CSS-based)](https://docs.crawl4ai.com/core/quickstart/#5-simple-data-extraction-css-based)\n  * [6. Simple Data Extraction (LLM-based)](https://docs.crawl4ai.com/core/quickstart/#6-simple-data-extraction-llm-based)\n  * [7. Multi-URL Concurrency (Preview)](https://docs.crawl4ai.com/core/quickstart/#7-multi-url-concurrency-preview)\n  * [8. Dynamic Content Example](https://docs.crawl4ai.com/core/quickstart/#8-dynamic-content-example)\n  * [9. Next Steps](https://docs.crawl4ai.com/core/quickstart/#9-next-steps)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "https",
        "crawl",
        "crawling",
        "crawled",
        "extraction",
        "extract",
        "extractions",
        "extracting",
        "extracted",
        "ai",
        "import",
        "important",
        "core",
        "result",
        "results",
        "content",
        "com",
        "page",
        "pages",
        "schemas",
        "schema",
        "async"
      ]
    },
    "https://docs.crawl4ai.com/core/installation": {
      "source_url": "https://docs.crawl4ai.com/core/installation",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/core/installation/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * Installation\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n  * [Installation & Setup (2023 Edition)](https://docs.crawl4ai.com/core/installation/#installation-setup-2023-edition)\n  * [1. Basic Installation](https://docs.crawl4ai.com/core/installation/#1-basic-installation)\n  * [2. Initial Setup & Diagnostics](https://docs.crawl4ai.com/core/installation/#2-initial-setup-diagnostics)\n  * [3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor)](https://docs.crawl4ai.com/core/installation/#3-verifying-installation-a-simple-crawl-skip-this-step-if-you-already-run-crawl4ai-doctor)\n  * [4. Advanced Installation (Optional)](https://docs.crawl4ai.com/core/installation/#4-advanced-installation-optional)\n  * [5. Docker (Experimental)](https://docs.crawl4ai.com/core/installation/#5-docker-experimental)\n  * [6. Local Server Mode (Legacy)](https://docs.crawl4ai.com/core/installation/#6-local-server-mode-legacy)\n  * [Summary](https://docs.crawl4ai.com/core/installation/#summary)\n\n\n# Installation & Setup (2023 Edition)\n## 1. Basic Installation\n```\npip install crawl4ai\nCopy\n```\n\nThis installs the **core** Crawl4AI library along with essential dependencies. **No** advanced features (like transformers or PyTorch) are included yet.\n## 2. Initial Setup & Diagnostics\n### 2.1 Run the Setup Command\nAfter installing, call:\n```\ncrawl4ai-setup\nCopy\n```\n\n**What does it do?** - Installs or updates required Playwright browsers (Chromium, Firefox, etc.) - Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl\n### 2.2 Diagnostics\nOptionally, you can run **diagnostics** to confirm everything is functioning:\n```\ncrawl4ai-doctor\nCopy\n```\n\nThis command attempts to: - Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts\nIf any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run `crawl4ai-setup`.\n## 3. Verifying Installation: A Simple Crawl (Skip this step if you already run `crawl4ai-doctor`)\nBelow is a minimal Python script demonstrating a **basic** crawl. It uses our new **`BrowserConfig`**and**`CrawlerRunConfig`**for clarity, though no custom settings are passed in this example:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\nasync def main():\n  async with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n      url=\"https://www.example.com\",\n    )\n    print(result.markdown[:300]) # Show the first 300 characters of extracted text\nif __name__ == \"__main__\":\n  asyncio.run(main())\nCopy\n```\n\n**Expected** outcome: - A headless browser session loads `example.com` - Crawl4AI returns ~300 characters of markdown. If errors occur, rerun `crawl4ai-doctor` or manually ensure Playwright is installed correctly.\n## 4. Advanced Installation (Optional)\n**Warning** : Only install these **if you truly need them**. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly.\n### 4.1 Torch, Transformers, or All\n  * **Text Clustering (Torch)**\n```\npip install crawl4ai[torch]\ncrawl4ai-setup\nCopy\n```\n\nInstalls PyTorch-based features (e.g., cosine similarity or advanced semantic chunking). \n  * **Transformers**\n```\npip install crawl4ai[transformer]\ncrawl4ai-setup\nCopy\n```\n\nAdds Hugging Face-based summarization or generation strategies. \n  * **All Features**\n```\npip install crawl4ai[all]\ncrawl4ai-setup\nCopy\n```\n\n\n\n#### (Optional) Pre-Fetching Models\n```\ncrawl4ai-download-models\nCopy\n```\n\nThis step caches large models locally (if needed). **Only do this** if your workflow requires them. \n## 5. Docker (Experimental)\nWe provide a **temporary** Docker approach for testing. **Itâ€™s not stable and may break** with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try:\n```\ndocker pull unclecode/crawl4ai:basic\ndocker run -p 11235:11235 unclecode/crawl4ai:basic\nCopy\n```\n\nYou can then make POST requests to `http://localhost:11235/crawl` to perform crawls. **Production usage** is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025).\n## 6. Local Server Mode (Legacy)\nSome older docs mention running Crawl4AI as a local server. This approach has been **partially replaced** by the new Docker-based prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized.\n## Summary\n1. **Install** with `pip install crawl4ai` and run `crawl4ai-setup`. 2. **Diagnose** with `crawl4ai-doctor` if you see errors. 3. **Verify** by crawling `example.com` with minimal `BrowserConfig` + `CrawlerRunConfig`. 4. **Advanced** features (Torch, Transformers) are **optional** â€”avoid them if you donâ€™t need them (they significantly increase resource usage). 5. **Docker** is **experimental** â€”use at your own risk until the stable version is released. 6. **Local server** references in older docs are largely deprecated; a new solution is in progress.\n**Got questions?** Check [GitHub issues](https://github.com/unclecode/crawl4ai/issues) for updates or ask the community!\n#### On this page\n  * [1. Basic Installation](https://docs.crawl4ai.com/core/installation/#1-basic-installation)\n  * [2. Initial Setup & Diagnostics](https://docs.crawl4ai.com/core/installation/#2-initial-setup-diagnostics)\n  * [2.1 Run the Setup Command](https://docs.crawl4ai.com/core/installation/#21-run-the-setup-command)\n  * [2.2 Diagnostics](https://docs.crawl4ai.com/core/installation/#22-diagnostics)\n  * [3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor)](https://docs.crawl4ai.com/core/installation/#3-verifying-installation-a-simple-crawl-skip-this-step-if-you-already-run-crawl4ai-doctor)\n  * [4. Advanced Installation (Optional)](https://docs.crawl4ai.com/core/installation/#4-advanced-installation-optional)\n  * [4.1 Torch, Transformers, or All](https://docs.crawl4ai.com/core/installation/#41-torch-transformers-or-all)\n  * [(Optional) Pre-Fetching Models](https://docs.crawl4ai.com/core/installation/#optional-pre-fetching-models)\n  * [5. Docker (Experimental)](https://docs.crawl4ai.com/core/installation/#5-docker-experimental)\n  * [6. Local Server Mode (Legacy)](https://docs.crawl4ai.com/core/installation/#6-local-server-mode-legacy)\n  * [Summary](https://docs.crawl4ai.com/core/installation/#summary)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "https",
        "crawl",
        "crawling",
        "crawls",
        "installation",
        "install",
        "installs",
        "installing",
        "installed",
        "core",
        "docker",
        "ai",
        "advanced",
        "com",
        "local",
        "locally",
        "copy",
        "setup",
        "models"
      ]
    },
    "https://docs.crawl4ai.com/core/examples": {
      "source_url": "https://docs.crawl4ai.com/core/examples",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/core/examples/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * Code Examples\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/#code-examples)\n  * [Getting Started Examples](https://docs.crawl4ai.com/core/examples/#getting-started-examples)\n  * [Browser & Crawling Features](https://docs.crawl4ai.com/core/examples/#browser-crawling-features)\n  * [Advanced Crawling & Deep Crawling](https://docs.crawl4ai.com/core/examples/#advanced-crawling-deep-crawling)\n  * [Extraction Strategies](https://docs.crawl4ai.com/core/examples/#extraction-strategies)\n  * [E-commerce & Specialized Crawling](https://docs.crawl4ai.com/core/examples/#e-commerce-specialized-crawling)\n  * [Customization & Security](https://docs.crawl4ai.com/core/examples/#customization-security)\n  * [Docker & Deployment](https://docs.crawl4ai.com/core/examples/#docker-deployment)\n  * [Application Examples](https://docs.crawl4ai.com/core/examples/#application-examples)\n  * [Content Generation & Markdown](https://docs.crawl4ai.com/core/examples/#content-generation-markdown)\n  * [Running the Examples](https://docs.crawl4ai.com/core/examples/#running-the-examples)\n  * [Contributing New Examples](https://docs.crawl4ai.com/core/examples/#contributing-new-examples)\n\n\n# Code Examples\nThis page provides a comprehensive list of example scripts that demonstrate various features and capabilities of Crawl4AI. Each example is designed to showcase specific functionality, making it easier for you to understand how to implement these features in your own projects.\n## Getting Started Examples\nExample | Description | Link  \n---|---|---  \nHello World | A simple introductory example demonstrating basic usage of AsyncWebCrawler with JavaScript execution and content filtering. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/hello_world.py)  \nQuickstart | A comprehensive collection of examples showcasing various features including basic crawling, content cleaning, link analysis, JavaScript execution, CSS selectors, media handling, custom hooks, proxy configuration, screenshots, and multiple extraction strategies. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.py)  \nQuickstart Set 1 | Basic examples for getting started with Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart_examples_set_1.py)  \nQuickstart Set 2 | More advanced examples for working with Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart_examples_set_2.py)  \n## Browser & Crawling Features\nExample | Description | Link  \n---|---|---  \nBuilt-in Browser | Demonstrates how to use the built-in browser capabilities. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/builtin_browser_example.py)  \nBrowser Optimization | Focuses on browser performance optimization techniques. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/browser_optimization_example.py)  \narun vs arun_many | Compares the `arun` and `arun_many` methods for single vs. multiple URL crawling. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/arun_vs_arun_many.py)  \nMultiple URLs | Shows how to crawl multiple URLs asynchronously. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/async_webcrawler_multiple_urls_example.py)  \nPage Interaction | Guide on interacting with dynamic elements through clicks. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md)  \nCrawler Monitor | Shows how to monitor the crawler's activities and status. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/crawler_monitor_example.py)  \nFull Page Screenshot & PDF | Guide on capturing full-page screenshots and PDFs from massive webpages. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/full_page_screenshot_and_pdf_export.md)  \n## Advanced Crawling & Deep Crawling\nExample | Description | Link  \n---|---|---  \nDeep Crawling | An extensive tutorial on deep crawling capabilities, demonstrating BFS and BestFirst strategies, stream vs. non-stream execution, filters, scorers, and advanced configurations. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/deepcrawl_example.py)  \nDispatcher | Shows how to use the crawl dispatcher for advanced workload management. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/dispatcher_example.py)  \nStorage State | Tutorial on managing browser storage state for persistence. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md)  \nNetwork Console Capture | Demonstrates how to capture and analyze network requests and console logs. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/network_console_capture_example.py)  \n## Extraction Strategies\nExample | Description | Link  \n---|---|---  \nExtraction Strategies | Demonstrates different extraction strategies with various input formats (markdown, HTML, fit_markdown) and JSON-based extractors (CSS and XPath). | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/extraction_strategies_examples.py)  \nScraping Strategies | Compares the performance of different scraping strategies. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/scraping_strategies_performance.py)  \nLLM Extraction | Demonstrates LLM-based extraction specifically for OpenAI pricing data. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/llm_extraction_openai_pricing.py)  \nLLM Markdown | Shows how to use LLMs to generate markdown from crawled content. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/llm_markdown_generator.py)  \nSummarize Page | Shows how to summarize web page content. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/summarize_page.py)  \n## E-commerce & Specialized Crawling\nExample | Description | Link  \n---|---|---  \nAmazon Product Extraction | Demonstrates how to extract structured product data from Amazon search results using CSS selectors. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/amazon_product_extraction_direct_url.py)  \nAmazon with Hooks | Shows how to use hooks with Amazon product extraction. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/amazon_product_extraction_using_hooks.py)  \nAmazon with JavaScript | Demonstrates using custom JavaScript for Amazon product extraction. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/amazon_product_extraction_using_use_javascript.py)  \nCrypto Analysis | Demonstrates how to crawl and analyze cryptocurrency data. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/crypto_analysis_example.py)  \nSERP API | Demonstrates using Crawl4AI with search engine result pages. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/serp_api_project_11_feb.py)  \n## Customization & Security\nExample | Description | Link  \n---|---|---  \nHooks | Illustrates how to use hooks at different stages of the crawling process for advanced customization. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/hooks_example.py)  \nIdentity-Based Browsing | Illustrates identity-based browsing configurations for authentic browsing experiences. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/identity_based_browsing.py)  \nProxy Rotation | Shows how to use proxy rotation for web scraping and avoiding IP blocks. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/proxy_rotation_demo.py)  \nSSL Certificate | Illustrates SSL certificate handling and verification. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/ssl_example.py)  \nLanguage Support | Shows how to handle different languages during crawling. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/language_support_example.py)  \nGeolocation | Demonstrates how to use geolocation features. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/use_geo_location.py)  \n## Docker & Deployment\nExample | Description | Link  \n---|---|---  \nDocker Config | Demonstrates how to create and use Docker configuration objects. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_config_obj.py)  \nDocker Basic | A test suite for Docker deployment, showcasing various functionalities through the Docker API. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_example.py)  \nDocker REST API | Shows how to interact with Crawl4AI Docker using REST API calls. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_python_rest_api.py)  \nDocker SDK | Demonstrates using the Python SDK for Crawl4AI Docker. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_python_sdk.py)  \n## Application Examples\nExample | Description | Link  \n---|---|---  \nResearch Assistant | Demonstrates how to build a research assistant using Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/research_assistant.py)  \nREST Call | Shows how to make REST API calls with Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/rest_call.py)  \nChainlit Integration | Shows how to integrate Crawl4AI with Chainlit. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/chainlit.md)  \nCrawl4AI vs FireCrawl | Compares Crawl4AI with the FireCrawl library. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/crawlai_vs_firecrawl.py)  \n## Content Generation & Markdown\nExample | Description | Link  \n---|---|---  \nContent Source | Demonstrates how to work with different content sources in markdown generation. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/markdown/content_source_example.py)  \nContent Source (Short) | A simplified version of content source usage. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/markdown/content_source_short_example.py)  \nBuilt-in Browser Guide | Guide for using the built-in browser capabilities. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md)  \n## Running the Examples\nTo run any of these examples, you'll need to have Crawl4AI installed:\n```\npip install crawl4ai\nCopy\n```\n\nThen, you can run an example script like this:\n```\npython -m docs.examples.hello_world\nCopy\n```\n\nFor examples that require additional dependencies or environment variables, refer to the comments at the top of each file.\nSome examples may require: - API keys (for LLM-based examples) - Docker setup (for Docker-related examples) - Additional dependencies (specified in the example files)\n## Contributing New Examples\nIf you've created an interesting example that demonstrates a unique use case or feature of Crawl4AI, we encourage you to contribute it to our examples collection. Please see our [contribution guidelines](https://github.com/unclecode/crawl4ai/blob/main/CONTRIBUTORS.md) for more information.\n#### On this page\n  * [Getting Started Examples](https://docs.crawl4ai.com/core/examples/#getting-started-examples)\n  * [Browser & Crawling Features](https://docs.crawl4ai.com/core/examples/#browser-crawling-features)\n  * [Advanced Crawling & Deep Crawling](https://docs.crawl4ai.com/core/examples/#advanced-crawling-deep-crawling)\n  * [Extraction Strategies](https://docs.crawl4ai.com/core/examples/#extraction-strategies)\n  * [E-commerce & Specialized Crawling](https://docs.crawl4ai.com/core/examples/#e-commerce-specialized-crawling)\n  * [Customization & Security](https://docs.crawl4ai.com/core/examples/#customization-security)\n  * [Docker & Deployment](https://docs.crawl4ai.com/core/examples/#docker-deployment)\n  * [Application Examples](https://docs.crawl4ai.com/core/examples/#application-examples)\n  * [Content Generation & Markdown](https://docs.crawl4ai.com/core/examples/#content-generation-markdown)\n  * [Running the Examples](https://docs.crawl4ai.com/core/examples/#running-the-examples)\n  * [Contributing New Examples](https://docs.crawl4ai.com/core/examples/#contributing-new-examples)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "examples",
        "example",
        "py",
        "https",
        "view",
        "crawl",
        "crawling",
        "crawled",
        "core",
        "strategies",
        "api",
        "advanced",
        "demonstrate",
        "demonstrating",
        "demonstrates",
        "extraction",
        "extract",
        "docker"
      ]
    },
    "https://docs.crawl4ai.com/core/llmtxt": {
      "source_url": "https://docs.crawl4ai.com/core/llmtxt",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/core/llmtxt/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * LLM Context\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "https",
        "core",
        "crawl",
        "crawling",
        "advanced",
        "ai",
        "com",
        "strategies",
        "api",
        "llm",
        "search",
        "searching",
        "files",
        "file",
        "markdown"
      ]
    },
    "https://docs.crawl4ai.com/core/ask-ai": {
      "source_url": "https://docs.crawl4ai.com/core/ask-ai",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/core/ask-ai/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * Ask AI\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "https",
        "core",
        "advanced",
        "crawl",
        "crawling",
        "ai",
        "strategies",
        "com",
        "api",
        "llm",
        "files",
        "file",
        "search",
        "searching",
        "markdown"
      ]
    },
    "https://docs.crawl4ai.com/core/cli": {
      "source_url": "https://docs.crawl4ai.com/core/cli",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/core/cli/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * Command Line Interface\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n  * [Crawl4AI CLI Guide](https://docs.crawl4ai.com/core/cli/#crawl4ai-cli-guide)\n  * [Table of Contents](https://docs.crawl4ai.com/core/cli/#table-of-contents)\n  * [Basic Usage](https://docs.crawl4ai.com/core/cli/#basic-usage)\n  * [Quick Example of Advanced Usage](https://docs.crawl4ai.com/core/cli/#quick-example-of-advanced-usage)\n  * [Configuration](https://docs.crawl4ai.com/core/cli/#configuration)\n  * [Advanced Features](https://docs.crawl4ai.com/core/cli/#advanced-features)\n  * [Output Formats](https://docs.crawl4ai.com/core/cli/#output-formats)\n  * [Complete Examples](https://docs.crawl4ai.com/core/cli/#complete-examples)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/cli/#best-practices-tips)\n  * [Recap](https://docs.crawl4ai.com/core/cli/#recap)\n\n\n# Crawl4AI CLI Guide\n## Table of Contents\n  * [Installation](https://docs.crawl4ai.com/core/cli/#installation)\n  * [Basic Usage](https://docs.crawl4ai.com/core/cli/#basic-usage)\n  * [Configuration](https://docs.crawl4ai.com/core/cli/#configuration)\n  * [Browser Configuration](https://docs.crawl4ai.com/core/cli/#browser-configuration)\n  * [Crawler Configuration](https://docs.crawl4ai.com/core/cli/#crawler-configuration)\n  * [Extraction Configuration](https://docs.crawl4ai.com/core/cli/#extraction-configuration)\n  * [Content Filtering](https://docs.crawl4ai.com/core/cli/#content-filtering)\n  * [Advanced Features](https://docs.crawl4ai.com/core/cli/#advanced-features)\n  * [LLM Q&A](https://docs.crawl4ai.com/core/cli/#llm-qa)\n  * [Structured Data Extraction](https://docs.crawl4ai.com/core/cli/#structured-data-extraction)\n  * [Content Filtering](https://docs.crawl4ai.com/core/cli/#content-filtering-1)\n  * [Output Formats](https://docs.crawl4ai.com/core/cli/#output-formats)\n  * [Examples](https://docs.crawl4ai.com/core/cli/#examples)\n  * [Configuration Reference](https://docs.crawl4ai.com/core/cli/#configuration-reference)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/cli/#best-practices--tips)\n\n\n## Basic Usage\nThe Crawl4AI CLI (`crwl`) provides a simple interface to the Crawl4AI library:\n```\n# Basic crawling\ncrwl https://example.com\n# Get markdown output\ncrwl https://example.com -o markdown\n# Verbose JSON output with cache bypass\ncrwl https://example.com -o json -v --bypass-cache\n# See usage examples\ncrwl --example\nCopy\n```\n\n## Quick Example of Advanced Usage\nIf you clone the repository and run the following command, you will receive the content of the page in JSON format according to a JSON-CSS schema:\n```\ncrwl \"https://www.infoq.com/ai-ml-data-eng/\" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;\nCopy\n```\n\n## Configuration\n### Browser Configuration\nBrowser settings can be configured via YAML file or command line parameters:\n```\n# browser.yml\nheadless: true\nviewport_width: 1280\nuser_agent_mode: \"random\"\nverbose: true\nignore_https_errors: true\nCopy\n```\n\n```\n# Using config file\ncrwl https://example.com -B browser.yml\n# Using direct parameters\ncrwl https://example.com -b \"headless=true,viewport_width=1280,user_agent_mode=random\"\nCopy\n```\n\n### Crawler Configuration\nControl crawling behavior:\n```\n# crawler.yml\ncache_mode: \"bypass\"\nwait_until: \"networkidle\"\npage_timeout: 30000\ndelay_before_return_html: 0.5\nword_count_threshold: 100\nscan_full_page: true\nscroll_delay: 0.3\nprocess_iframes: false\nremove_overlay_elements: true\nmagic: true\nverbose: true\nCopy\n```\n\n```\n# Using config file\ncrwl https://example.com -C crawler.yml\n# Using direct parameters\ncrwl https://example.com -c \"css_selector=#main,delay_before_return_html=2,scan_full_page=true\"\nCopy\n```\n\n### Extraction Configuration\nTwo types of extraction are supported:\n  1. CSS/XPath-based extraction: \n```\n# extract_css.yml\ntype: \"json-css\"\nparams:\n verbose: true\nCopy\n```\n\n\n\n```\n// css_schema.json\n{\n \"name\": \"ArticleExtractor\",\n \"baseSelector\": \".article\",\n \"fields\": [\n  {\n   \"name\": \"title\",\n   \"selector\": \"h1.title\",\n   \"type\": \"text\"\n  },\n  {\n   \"name\": \"link\",\n   \"selector\": \"a.read-more\",\n   \"type\": \"attribute\",\n   \"attribute\": \"href\"\n  }\n ]\n}\nCopy\n```\n\n  1. LLM-based extraction: \n```\n# extract_llm.yml\ntype: \"llm\"\nprovider: \"openai/gpt-4\"\ninstruction: \"Extract all articles with their titles and links\"\napi_token: \"your-token\"\nparams:\n temperature: 0.3\n max_tokens: 1000\nCopy\n```\n\n\n\n```\n// llm_schema.json\n{\n \"title\": \"Article\",\n \"type\": \"object\",\n \"properties\": {\n  \"title\": {\n   \"type\": \"string\",\n   \"description\": \"The title of the article\"\n  },\n  \"link\": {\n   \"type\": \"string\",\n   \"description\": \"URL to the full article\"\n  }\n }\n}\nCopy\n```\n\n## Advanced Features\n### LLM Q&A\nAsk questions about crawled content:\n```\n# Simple question\ncrwl https://example.com -q \"What is the main topic discussed?\"\n# View content then ask questions\ncrwl https://example.com -o markdown # See content first\ncrwl https://example.com -q \"Summarize the key points\"\ncrwl https://example.com -q \"What are the conclusions?\"\n# Combined with advanced crawling\ncrwl https://example.com \\\n  -B browser.yml \\\n  -c \"css_selector=article,scan_full_page=true\" \\\n  -q \"What are the pros and cons mentioned?\"\nCopy\n```\n\nFirst-time setup: - Prompts for LLM provider and API token - Saves configuration in `~/.crawl4ai/global.yml` - Supports various providers (openai/gpt-4, anthropic/claude-3-sonnet, etc.) - For case of `ollama` you do not need to provide API token. - See [LiteLLM Providers](https://docs.litellm.ai/docs/providers) for full list\n### Structured Data Extraction\nExtract structured data using CSS selectors:\n```\ncrwl https://example.com \\\n  -e extract_css.yml \\\n  -s css_schema.json \\\n  -o json\nCopy\n```\n\nOr using LLM-based extraction:\n```\ncrwl https://example.com \\\n  -e extract_llm.yml \\\n  -s llm_schema.json \\\n  -o json\nCopy\n```\n\n### Content Filtering\nFilter content for relevance:\n```\n# filter_bm25.yml\ntype: \"bm25\"\nquery: \"target content\"\nthreshold: 1.0\n# filter_pruning.yml\ntype: \"pruning\"\nquery: \"focus topic\"\nthreshold: 0.48\nCopy\n```\n\n```\ncrwl https://example.com -f filter_bm25.yml -o markdown-fit\nCopy\n```\n\n## Output Formats\n  * `all` - Full crawl result including metadata\n  * `json` - Extracted structured data (when using extraction)\n  * `markdown` / `md` - Raw markdown output\n  * `markdown-fit` / `md-fit` - Filtered markdown for better readability\n\n\n## Complete Examples\n  1. Basic Extraction: \n```\ncrwl https://example.com \\\n  -B browser.yml \\\n  -C crawler.yml \\\n  -o json\nCopy\n```\n\n  2. Structured Data Extraction: \n```\ncrwl https://example.com \\\n  -e extract_css.yml \\\n  -s css_schema.json \\\n  -o json \\\n  -v\nCopy\n```\n\n  3. LLM Extraction with Filtering: \n```\ncrwl https://example.com \\\n  -B browser.yml \\\n  -e extract_llm.yml \\\n  -s llm_schema.json \\\n  -f filter_bm25.yml \\\n  -o json\nCopy\n```\n\n  4. Interactive Q&A: \n```\n# First crawl and view\ncrwl https://example.com -o markdown\n# Then ask questions\ncrwl https://example.com -q \"What are the main points?\"\ncrwl https://example.com -q \"Summarize the conclusions\"\nCopy\n```\n\n\n\n## Best Practices & Tips\n  1. **Configuration Management** :\n  2. Keep common configurations in YAML files\n  3. Use CLI parameters for quick overrides\n  4. Store sensitive data (API tokens) in `~/.crawl4ai/global.yml`\n  5. **Performance Optimization** :\n  6. Use `--bypass-cache` for fresh content\n  7. Enable `scan_full_page` for infinite scroll pages\n  8. Adjust `delay_before_return_html` for dynamic content\n  9. **Content Extraction** :\n  10. Use CSS extraction for structured content\n  11. Use LLM extraction for unstructured content\n  12. Combine with filters for focused results\n  13. **Q &A Workflow**:\n  14. View content first with `-o markdown`\n  15. Ask specific questions\n  16. Use broader context with appropriate selectors\n\n\n## Recap\nThe Crawl4AI CLI provides: - Flexible configuration via files and parameters - Multiple extraction strategies (CSS, XPath, LLM) - Content filtering and optimization - Interactive Q&A capabilities - Various output formats\n#### On this page\n  * [Table of Contents](https://docs.crawl4ai.com/core/cli/#table-of-contents)\n  * [Basic Usage](https://docs.crawl4ai.com/core/cli/#basic-usage)\n  * [Quick Example of Advanced Usage](https://docs.crawl4ai.com/core/cli/#quick-example-of-advanced-usage)\n  * [Configuration](https://docs.crawl4ai.com/core/cli/#configuration)\n  * [Browser Configuration](https://docs.crawl4ai.com/core/cli/#browser-configuration)\n  * [Crawler Configuration](https://docs.crawl4ai.com/core/cli/#crawler-configuration)\n  * [Extraction Configuration](https://docs.crawl4ai.com/core/cli/#extraction-configuration)\n  * [Advanced Features](https://docs.crawl4ai.com/core/cli/#advanced-features)\n  * [LLM Q&A](https://docs.crawl4ai.com/core/cli/#llm-qa)\n  * [Structured Data Extraction](https://docs.crawl4ai.com/core/cli/#structured-data-extraction)\n  * [Content Filtering](https://docs.crawl4ai.com/core/cli/#content-filtering)\n  * [Output Formats](https://docs.crawl4ai.com/core/cli/#output-formats)\n  * [Complete Examples](https://docs.crawl4ai.com/core/cli/#complete-examples)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/cli/#best-practices-tips)\n  * [Recap](https://docs.crawl4ai.com/core/cli/#recap)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "https",
        "copy",
        "cli",
        "content",
        "contents",
        "extraction",
        "extract",
        "extracted",
        "crawl",
        "crawling",
        "crawled",
        "core",
        "advanced",
        "com",
        "llm",
        "yml",
        "configuration",
        "configured",
        "configurations"
      ]
    },
    "https://docs.crawl4ai.com/blog": {
      "source_url": "https://docs.crawl4ai.com/blog",
      "content": "[Crawl4AI Documentation (v0.6.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ LLM Context ](https://docs.crawl4ai.com/core/llmtxt/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Search ](https://docs.crawl4ai.com/blog/)\n\n\n[ unclecode/crawl4ai 46.5k 4.4k ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [LLM Context](https://docs.crawl4ai.com/core/llmtxt/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Docker Deployment](https://docs.crawl4ai.com/core/docker-deployment/)\n  * Blog & Changelog\n    * Blog Home\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n\n\n  * [Crawl4AI Blog](https://docs.crawl4ai.com/blog/#crawl4ai-blog)\n  * [Latest Release](https://docs.crawl4ai.com/blog/#latest-release)\n  * [License Change](https://docs.crawl4ai.com/blog/#license-change)\n  * [Project History](https://docs.crawl4ai.com/blog/#project-history)\n  * [Stay Updated](https://docs.crawl4ai.com/blog/#stay-updated)\n\n\n# Crawl4AI Blog\nWelcome to the Crawl4AI blog! Here you'll find detailed release notes, technical insights, and updates about the project. Whether you're looking for the latest improvements or want to dive deep into web crawling techniques, this is the place.\n## Latest Release\nHereâ€™s the blog index entry for **v0.6.0** , written to match the exact tone and structure of your previous entries:\n### [Crawl4AI v0.6.0 â€“ World-Aware Crawling, Pre-Warmed Browsers, and the MCP API](https://docs.crawl4ai.com/blog/releases/0.6.0/)\n_April 23, 2025_\nCrawl4AI v0.6.0 is our most powerful release yet. This update brings major architectural upgrades including world-aware crawling (set geolocation, locale, and timezone), real-time traffic capture, and a memory-efficient crawler pool with pre-warmed pages. \nThe Docker server now exposes a full-featured MCP socket + SSE interface, supports streaming, and comes with a new Playground UI. Plus, table extraction is now native, and the new stress-test framework supports crawling 1,000+ URLs. \nOther key changes: \n  * Native support for `result.media[\"tables\"]` to export DataFrames \n  * Full network + console logs and MHTML snapshot per crawl \n  * Browser pooling and pre-warming for faster cold starts \n  * New streaming endpoints via MCP API and Playground \n  * Robots.txt support, proxy rotation, and improved session handling \n  * Deprecated old markdown names, legacy modules cleaned up \n  * Massive repo cleanup: ~36K insertions, ~5K deletions across 121 files\n\n\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/releases/0.6.0/)\nLet me know if you want me to auto-update the actual file or just paste this into the markdown.\n### [Crawl4AI v0.5.0: Deep Crawling, Scalability, and a New CLI!](https://docs.crawl4ai.com/blog/releases/0.5.0/)\nMy dear friends and crawlers, there you go, this is the release of Crawl4AI v0.5.0! This release brings a wealth of new features, performance improvements, and a more streamlined developer experience. Here's a breakdown of what's new:\n**Major New Features:**\n  * **Deep Crawling:** Explore entire websites with configurable strategies (BFS, DFS, Best-First). Define custom filters and URL scoring for targeted crawls.\n  * **Memory-Adaptive Dispatcher:** Handle large-scale crawls with ease! Our new dispatcher dynamically adjusts concurrency based on available memory and includes built-in rate limiting.\n  * **Multiple Crawler Strategies:** Choose between the full-featured Playwright browser-based crawler or a new, _much_ faster HTTP-only crawler for simpler tasks.\n  * **Docker Deployment:** Deploy Crawl4AI as a scalable, self-contained service with built-in API endpoints and optional JWT authentication.\n  * **Command-Line Interface (CLI):** Interact with Crawl4AI directly from your terminal. Crawl, configure, and extract data with simple commands.\n  * **LLM Configuration (`LLMConfig`):** A new, unified way to configure LLM providers (OpenAI, Anthropic, Ollama, etc.) for extraction, filtering, and schema generation. Simplifies API key management and switching between models.\n\n\n**Minor Updates & Improvements:**\n  * **LXML Scraping Mode:** Faster HTML parsing with `LXMLWebScrapingStrategy`.\n  * **Proxy Rotation:** Added `ProxyRotationStrategy` with a `RoundRobinProxyStrategy` implementation.\n  * **PDF Processing:** Extract text, images, and metadata from PDF files.\n  * **URL Redirection Tracking:** Automatically follows and records redirects.\n  * **Robots.txt Compliance:** Optionally respect website crawling rules.\n  * **LLM-Powered Schema Generation:** Automatically create extraction schemas using an LLM.\n  * **`LLMContentFilter`:** Generate high-quality, focused markdown using an LLM.\n  * **Improved Error Handling & Stability:** Numerous bug fixes and performance enhancements.\n  * **Enhanced Documentation:** Updated guides and examples.\n\n\n**Breaking Changes & Migration:**\nThis release includes several breaking changes to improve the library's structure and consistency. Here's what you need to know:\n  * **`arun_many()`Behavior:** Now uses the `MemoryAdaptiveDispatcher` by default. The return type depends on the `stream` parameter in `CrawlerRunConfig`. Adjust code that relied on unbounded concurrency.\n  * **`max_depth`Location:** Moved to `CrawlerRunConfig` and now controls _crawl depth_.\n  * **Deep Crawling Imports:** Import `DeepCrawlStrategy` and related classes from `crawl4ai.deep_crawling`.\n  * **`BrowserContext`API:** Updated; the old `get_context` method is deprecated.\n  * **Optional Model Fields:** Many data model fields are now optional. Handle potential `None` values.\n  * **`ScrapingMode`Enum:** Replaced with strategy pattern (`WebScrapingStrategy`, `LXMLWebScrapingStrategy`).\n  * **`content_filter`Parameter:** Removed from `CrawlerRunConfig`. Use extraction strategies or markdown generators with filters.\n  * **Removed Functionality:** The synchronous `WebCrawler`, the old CLI, and docs management tools have been removed.\n  * **Docker:** Significant changes to deployment. See the [Docker documentation](https://docs.crawl4ai.com/deploy/docker/README.md).\n  * **`ssl_certificate.json`:** This file has been removed.\n  * **Config** : FastFilterChain has been replaced with FilterChain\n  * **Deep-Crawl** : DeepCrawlStrategy.arun now returns Union[CrawlResultT, List[CrawlResultT], AsyncGenerator[CrawlResultT, None]]\n  * **Proxy** : Removed synchronous WebCrawler support and related rate limiting configurations\n  * **LLM Parameters:** Use the new `LLMConfig` object instead of passing `provider`, `api_token`, `base_url`, and `api_base` directly to `LLMExtractionStrategy` and `LLMContentFilter`.\n\n\n**In short:** Update imports, adjust `arun_many()` usage, check for optional fields, and review the Docker deployment guide.\n## License Change\nCrawl4AI v0.5.0 updates the license to Apache 2.0 _with a required attribution clause_. This means you are free to use, modify, and distribute Crawl4AI (even commercially), but you _must_ clearly attribute the project in any public use or distribution. See the updated `LICENSE` file for the full legal text and specific requirements.\n**Get Started:**\n  * **Installation:** `pip install \"crawl4ai[all]\"` (or use the Docker image)\n  * **Documentation:** <https://docs.crawl4ai.com>\n  * **GitHub:** <https://github.com/unclecode/crawl4ai>\n\n\nI'm very excited to see what you build with Crawl4AI v0.5.0!\n### [0.4.2 - Configurable Crawlers, Session Management, and Smarter Screenshots](https://docs.crawl4ai.com/blog/releases/0.4.2/)\n_December 12, 2024_\nThe 0.4.2 update brings massive improvements to configuration, making crawlers and browsers easier to manage with dedicated objects. You can now import/export local storage for seamless session management. Plus, long-page screenshots are faster and cleaner, and full-page PDF exports are now possible. Check out all the new features to make your crawling experience even smoother.\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/releases/0.4.2/)\n### [0.4.1 - Smarter Crawling with Lazy-Load Handling, Text-Only Mode, and More](https://docs.crawl4ai.com/blog/releases/0.4.1/)\n_December 8, 2024_\nThis release brings major improvements to handling lazy-loaded images, a blazing-fast Text-Only Mode, full-page scanning for infinite scrolls, dynamic viewport adjustments, and session reuse for efficient crawling. If you're looking to improve speed, reliability, or handle dynamic content with ease, this update has you covered.\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/releases/0.4.1/)\n### [0.4.0 - Major Content Filtering Update](https://docs.crawl4ai.com/blog/releases/0.4.0/)\n_December 1, 2024_\nIntroduced significant improvements to content filtering, multi-threaded environment handling, and user-agent generation. This release features the new PruningContentFilter, enhanced thread safety, and improved test coverage.\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/releases/0.4.0/)\n## Project History\nCurious about how Crawl4AI has evolved? Check out our [complete changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md) for a detailed history of all versions and updates.\n## Stay Updated\n  * Star us on [GitHub](https://github.com/unclecode/crawl4ai)\n  * Follow [@unclecode](https://twitter.com/unclecode) on Twitter\n  * Join our community discussions on GitHub\n\n\n#### On this page\n  * [Latest Release](https://docs.crawl4ai.com/blog/#latest-release)\n  * [Crawl4AI v0.6.0 â€“ World-Aware Crawling, Pre-Warmed Browsers, and the MCP API](https://docs.crawl4ai.com/blog/#crawl4ai-v060-world-aware-crawling-pre-warmed-browsers-and-the-mcp-api)\n  * [Crawl4AI v0.5.0: Deep Crawling, Scalability, and a New CLI!](https://docs.crawl4ai.com/blog/#crawl4ai-v050-deep-crawling-scalability-and-a-new-cli)\n  * [License Change](https://docs.crawl4ai.com/blog/#license-change)\n  * [0.4.2 - Configurable Crawlers, Session Management, and Smarter Screenshots](https://docs.crawl4ai.com/blog/#042-configurable-crawlers-session-management-and-smarter-screenshots)\n  * [0.4.1 - Smarter Crawling with Lazy-Load Handling, Text-Only Mode, and More](https://docs.crawl4ai.com/blog/#041-smarter-crawling-with-lazy-load-handling-text-only-mode-and-more)\n  * [0.4.0 - Major Content Filtering Update](https://docs.crawl4ai.com/blog/#040-major-content-filtering-update)\n  * [Project History](https://docs.crawl4ai.com/blog/#project-history)\n  * [Stay Updated](https://docs.crawl4ai.com/blog/#stay-updated)\n\n\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n",
      "embedding": null,
      "depth": 1,
      "score": 1.0,
      "keywords": [
        "crawl",
        "crawling",
        "crawls",
        "_crawl",
        "https",
        "core",
        "updated",
        "updates",
        "update",
        "release",
        "releases",
        "blog",
        "strategies",
        "strategy",
        "llm",
        "ai",
        "api",
        "extraction",
        "extract",
        "handling",
        "handle"
      ]
    }
  },
  "edges": [
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/blog",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "extraction",
        "crawl",
        "llm",
        "https",
        "crawling",
        "api",
        "core",
        "ai"
      ],
      "semantic_similarity": 0.38095238095238093
    },
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/core/cli",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "extraction",
        "extracted",
        "crawl",
        "llm",
        "com",
        "https",
        "crawling",
        "core",
        "advanced"
      ],
      "semantic_similarity": 0.47368421052631576
    },
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/core/ask-ai",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "crawl",
        "llm",
        "com",
        "https",
        "crawling",
        "api",
        "core",
        "ai",
        "advanced"
      ],
      "semantic_similarity": 0.6
    },
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/core/llmtxt",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "crawl",
        "llm",
        "com",
        "https",
        "crawling",
        "api",
        "core",
        "ai",
        "advanced"
      ],
      "semantic_similarity": 0.6
    },
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/core/examples",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "extraction",
        "crawl",
        "https",
        "crawling",
        "api",
        "core",
        "advanced"
      ],
      "semantic_similarity": 0.3888888888888889
    },
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/core/installation",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "crawl",
        "com",
        "https",
        "crawling",
        "core",
        "ai",
        "advanced"
      ],
      "semantic_similarity": 0.3684210526315789
    },
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/core/quickstart",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "extraction",
        "extracted",
        "crawl",
        "com",
        "https",
        "crawling",
        "core",
        "ai"
      ],
      "semantic_similarity": 0.36363636363636365
    },
    {
      "source": "https://docs.crawl4ai.com",
      "target": "https://docs.crawl4ai.com/core/docker-deployment",
      "relation_type": "NAVIGATES_TO",
      "weight": 0.8,
      "common_keywords": [
        "crawl",
        "https",
        "crawling",
        "api",
        "ai"
      ],
      "semantic_similarity": 0.19230769230769232
    }
  ],
  "metadata": {
    "total_nodes": 9,
    "max_depth": 1,
    "root_url": "https://docs.crawl4ai.com"
  }
}